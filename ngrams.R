files = list.files(path="E:/MainProject/sampletrainonlyBytes",pattern="*.bytes",full.names=T)
library(RWeka)
library(tm)
finalDt = data.frame()
n=2
i=0
ngramTokenizer = function(x) NGramTokenizer(x, Weka_control(min = n, max = n))
for(x in files){
  corpus = Corpus(URISource(x),readerControl=list(language="eng", reader=readPlain))
  #corpus = tm_map(corpus,tolower)
  #corpus = tm_map(corpus, PlainTextDocument)
  corpus = tm_map(corpus,removePunctuation)
  #corpus = tm_map(corpus,removeWords,stopwords("english"))
  #corpus = tm_map(corpus,stemDocument)
  dtm = DocumentTermMatrix(corpus, control = list(tokenize = ngramTokenizer))
  df = as.data.frame(as.matrix(dtm))
  if(i==0){
  finalDt = rbind(df,finalDt)
  i=1;
  }
  else{
    cols <- intersect(colnames(df), colnames(finalDt))
    finalDt =  rbind(df[,cols], finalDt[,cols])
  }
}

fileNames = list.files(path="E:/MainProject/sampletrainonlyBytes",pattern="*.bytes",full.names=F)
names = data.frame()
for(i in fileNames){
  names = rbind(names,as.data.frame(i))
}

colnames(finalDt) = make.names(colnames(finalDt))
#extract to csv and load back
features$X = substr(features$X,0,20)
labels = read.csv("trainLabels.csv")
l = merge(features,labels,by.x="X",by.y="Id")
colnames(finalDt) = make.names(colnames(finalDt))